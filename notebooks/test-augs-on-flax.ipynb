{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from flax import linen as nn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import nnaugment\n",
    "import numpy as np\n",
    "\n",
    "bias_init = nn.initializers.normal(stddev=1e-6)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), bias_init=bias_init)(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Conv(features=16, kernel_size=(3, 3), bias_init=bias_init)(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.flatten()\n",
    "        x = nn.Dense(features=15, name=\"Dense_2\", bias_init=bias_init)(x)\n",
    "        x = nn.Dense(features=10, name=\"Dense_3\", bias_init=bias_init)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(0)\n",
    "input_shape = (28, 28, 1)\n",
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights\n",
    "x = jnp.ones(input_shape)\n",
    "variables = model.init(rng, x)\n",
    "initial_output = model.apply(variables, x)\n",
    "\n",
    "\n",
    "# Augment weights\n",
    "if isinstance(model, SimpleCNN):\n",
    "    layers_to_permute = [\"Conv_0\", \"Conv_1\", \"Dense_2\"]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {type(model)}\")\n",
    "\n",
    "augmented_params = nnaugment.random_permutation(\n",
    "    variables['params'], \n",
    "    layers_to_permute=layers_to_permute,\n",
    "    convention=\"flax\")\n",
    "augmented_variables = {'params': augmented_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'Conv_0': {'bias': (32,), 'kernel': (3, 3, 1, 32)},\n",
       "  'Conv_1': {'bias': (16,), 'kernel': (3, 3, 32, 16)},\n",
       "  'Dense_2': {'bias': (15,), 'kernel': (3136, 15)},\n",
       "  'Dense_3': {'bias': (10,), 'kernel': (15, 10)}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(lambda x: x.shape, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-3.50728271e-07, -7.06043295e-07, -6.61901595e-07, -4.50394765e-07,\n",
       "       -1.64961222e-07,  1.16701165e-06, -1.41028158e-06, -5.46690103e-07,\n",
       "        6.42782652e-07, -2.02183230e-07, -1.48379002e-06, -1.33969615e-06,\n",
       "       -5.20142976e-07, -8.90838692e-07,  9.51444690e-07, -7.46093690e-07,\n",
       "        1.38884332e-06,  5.91921662e-07, -3.42096371e-07,  5.37360563e-07,\n",
       "        6.38460165e-07, -4.31932023e-07, -4.78100560e-07,  7.13702150e-07,\n",
       "        9.17730558e-10,  1.94411740e-07, -3.80567258e-07, -4.25100353e-07,\n",
       "       -2.56481769e-07,  1.69610064e-06, -9.35697869e-07, -1.08783155e-07],      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables['params']['Conv_0']['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Bias parameters of model SimpleCNN, layer Conv_1 are almost identical after augmentation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m layers_to_permute:\n\u001b[1;32m      4\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mallclose(layer[\u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m], variables[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m][name][\u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m], rtol\u001b[39m=\u001b[39m\u001b[39m5e-2\u001b[39m), \\\n\u001b[1;32m      5\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKernel parameters of model \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, layer \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m are almost identical after augmentation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mallclose(layer[\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m], variables[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m][name][\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m], rtol\u001b[39m=\u001b[39m\u001b[39m5e-2\u001b[39m), \\\n\u001b[1;32m      7\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBias parameters of model \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, layer \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m are almost identical after augmentation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mallclose(layer[\u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m], variables[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m][name][\u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m], rtol\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m), \\\n\u001b[1;32m     10\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKernel parameters of model \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, layer \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m are within +-20% size of each other after augmentation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mallclose(layer[\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m], variables[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m][name][\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m], rtol\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m), \\\n\u001b[1;32m     12\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBias parameters of model \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, layer \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m are within +-20% size of each other after augmentation.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Bias parameters of model SimpleCNN, layer Conv_1 are almost identical after augmentation."
     ]
    }
   ],
   "source": [
    "\n",
    "# Check non-equality of augmented model's parameters against the original\n",
    "for name, layer in augmented_params.items():\n",
    "    if name in layers_to_permute:\n",
    "        assert not np.allclose(layer[\"kernel\"], variables['params'][name][\"kernel\"], rtol=5e-2), \\\n",
    "            f\"Kernel parameters of model {type(model).__name__}, layer {name} are almost identical after augmentation.\"\n",
    "        assert not np.allclose(layer[\"bias\"], variables['params'][name][\"bias\"], rtol=5e-2), \\\n",
    "            f\"Bias parameters of model {type(model).__name__}, layer {name} are almost identical after augmentation.\"\n",
    "\n",
    "        assert not np.allclose(layer[\"kernel\"], variables['params'][name][\"kernel\"], rtol=0.2), \\\n",
    "            f\"Kernel parameters of model {type(model).__name__}, layer {name} are within +-20% size of each other after augmentation.\"\n",
    "        assert not np.allclose(layer[\"bias\"], variables['params'][name][\"bias\"], rtol=0.2), \\\n",
    "            f\"Bias parameters of model {type(model).__name__}, layer {name} are within +-20% size of each other after augmentation.\"\n",
    "\n",
    "# Check for unchanged output\n",
    "augmented_output = model.apply(augmented_variables, x)\n",
    "assert jnp.allclose(initial_output, augmented_output, atol=1e-6), \"Outputs differ after weight augmentation.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
